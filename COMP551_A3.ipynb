{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import csv\n",
    "import math\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing Data from Files\n",
    "def import_dataset(file):\n",
    "    raw_train, raw_valid, raw_test = [],[], []\n",
    "    Y_train, Y_valid, Y_test = [],[],[]\n",
    "    \n",
    "    train_str = \"./Datasets/\" + str(file) + \"-train.txt\"\n",
    "    valid_str = \"./Datasets/\" + str(file) + \"-valid.txt\"\n",
    "    test_str = \"./Datasets/\" + str(file) + \"-test.txt\"\n",
    "    \n",
    "    raw_train = [line.lower().rstrip('\\t') for line in open(train_str)]\n",
    "    raw_train = np.array(raw_train)\n",
    "    Y_train = np.zeros((len(raw_train)), dtype='int')\n",
    "    for i in range(len(raw_train)):\n",
    "        Y_train[i] = raw_train[i][-2:]\n",
    "        raw_train[i] = re.sub('[^A-Za-z\\s]+','',str(raw_train[i]))\n",
    "        raw_train[i] = raw_train[i][:-2]\n",
    "    \n",
    "    raw_valid = [line.lower().rstrip('\\t') for line in open(valid_str)]\n",
    "    raw_valid = np.array(raw_valid)\n",
    "    Y_valid = np.zeros((len(raw_valid)), dtype='int')\n",
    "    for i in range(len(raw_valid)):\n",
    "        Y_valid[i] = raw_valid[i][-2:]\n",
    "        raw_valid[i] = re.sub('[^A-Za-z\\s]+','',str(raw_valid[i]))\n",
    "        raw_valid[i] = raw_valid[i][:-2]\n",
    "\n",
    "    raw_test = [line.lower().rstrip('\\t') for line in open(test_str)]\n",
    "    raw_test = np.array(raw_test)\n",
    "    Y_test = np.zeros((len(raw_test)), dtype='int')\n",
    "    for i in range(len(raw_test)):\n",
    "        Y_test[i] = raw_test[i][-2:]\n",
    "        raw_test[i] = re.sub('[^A-Za-z\\s]+','',str(raw_test[i]))\n",
    "        raw_test[i] = raw_test[i][:-2]\n",
    "        \n",
    "    return raw_train, raw_valid, raw_test, Y_train, Y_valid, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing Data from files created by Bag of Words or Frequency Bag of Words\n",
    "def import_created_dataset(file):\n",
    "    X_train, X_valid, X_test = [],[], []\n",
    "    Y_train, Y_valid, Y_test = [],[],[]\n",
    "    \n",
    "    train_str = \"./Created_Datasets/\" + str(file) + \"_X_train.csv\"\n",
    "    valid_str = \"./Created_Datasets/\" + str(file) + \"_X_valid.csv\"\n",
    "    test_str = \"./Created_Datasets/\" + str(file) + \"_X_test.csv\"\n",
    "    \n",
    "    label_train_str =  \"./Created_Datasets/\" + str(file) + \"_Y_train.csv\"\n",
    "    label_valid_str = \"./Created_Datasets/\" + str(file) + \"_Y_valid.csv\"\n",
    "    label_test_str = \"./Created_Datasets/\" + str(file) + \"_Y_test.csv\"\n",
    "    \n",
    "    with open(train_str, 'rb') as csv_train_file:\n",
    "        train_data = csv.reader(csv_train_file, delimiter = ',')\n",
    "        for row in train_data:\n",
    "            X_train.append(row)\n",
    "        del(row)\n",
    "    \n",
    "    with open(valid_str, 'rb') as csv_valid_file:\n",
    "        valid_data = csv.reader(csv_valid_file, delimiter = ',')\n",
    "        for row in valid_data:\n",
    "            X_valid.append(row)\n",
    "        del(row)\n",
    "        \n",
    "    with open(test_str, 'rb') as csv_test_file:\n",
    "        test_data = csv.reader(csv_test_file, delimiter = ',')\n",
    "        for row in test_data:\n",
    "            X_test.append(row)\n",
    "        del(row)\n",
    "    \n",
    "    with open(label_train_str, 'rb') as label_csv_train_file:\n",
    "        label_train_data = csv.reader(label_csv_train_file, delimiter = ',')\n",
    "        for row in label_train_data:\n",
    "            Y_train.append(row)\n",
    "        del(row)\n",
    "    \n",
    "    with open(label_valid_str, 'rb') as label_csv_valid_file:\n",
    "        label_valid_data = csv.reader(label_csv_valid_file, delimiter = ',')\n",
    "        for row in label_valid_data:\n",
    "            Y_valid.append(row)\n",
    "        del(row)\n",
    "        \n",
    "    with open(label_test_str, 'rb') as label_csv_test_file:\n",
    "        label_test_data = csv.reader(label_csv_test_file, delimiter = ',')\n",
    "        for row in label_test_data:\n",
    "            Y_test.append(row)\n",
    "        del(row)\n",
    "        \n",
    "    return np.array(X_train), np.array(X_valid), np.array(X_test), np.array(Y_train), np.array(Y_valid), np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def export_vocab_file(dataset, file_name):\n",
    "    list_temp = []\n",
    "    MF_words = []\n",
    "    for row in dataset:\n",
    "        list_temp.extend(row.split(\" \"))\n",
    "    MF_tuple = Counter(list_temp).most_common(10001)\n",
    "    MF_words = [word for word, word_count in MF_tuple if word is not '']\n",
    "    MF_count = [word_count for word, word_count in MF_tuple if word is not '']\n",
    "    \n",
    "    data_str = []\n",
    "    for i in range(len(MF_words)):\n",
    "        count_str = str(MF_words[i]) + \" \\t\" + str(i) + \"\\t\" + str(MF_count[i])\n",
    "        data_str.append(count_str)\n",
    "    data_str = np.array(data_str, dtype = str)\n",
    "    np.savetxt(file_name, data_str, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method to find the 10,000 most frequent words\n",
    "def get_most_frequent_words(dataset):\n",
    "    list_temp = []\n",
    "    MF_words = []\n",
    "    for row in dataset:\n",
    "        list_temp.extend(row.split(\" \"))\n",
    "    MF_tuple = Counter(list_temp).most_common(10001)\n",
    "    MF_words = [word for word, word_count in MF_tuple if word is not '']\n",
    "    MF_count = [word_count for word, word_count in MF_tuple if word is not '']\n",
    "    print(\"Returning 10,000 most frequent\")\n",
    "    return (MF_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(dataset):\n",
    "    for review in dataset:\n",
    "        word_counter = Counter()\n",
    "    for review in dataset:\n",
    "        word_counter.update(review.split())\n",
    "    \n",
    "    v = word_counter.most_common(10000)\n",
    "    words=np.zeros((10000,2),dtype=\"S100\")\n",
    "\n",
    "    for w in range(len(v)):\n",
    "        words[w][0] = v[w][0]\n",
    "        words[w,1] = w\n",
    "\n",
    "    vocabulary = dict(words)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_dataset_review_indexes(dataset, file_name, vocab, y_values):\n",
    "    output_str = []\n",
    "    for sentence in range(len(dataset)):\n",
    "        string = \"\"\n",
    "        for index, word in enumerate(dataset[sentence].split()):\n",
    "            try:\n",
    "                word_id = vocab[word]\n",
    "                string = string + str(word_id) + \" \"\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        string = string + \"\\t\" + str(y_values[sentence]) \n",
    "        output_str.append(string)\n",
    "    \n",
    "    np.savetxt(file_name, np.array(output_str), fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_bag_of_words(dataset, size, vocab_size):  \n",
    "    fbow = frequency_bag_of_words(dataset, size, vocab_size)\n",
    "    bbow = np.copy(fbow)\n",
    "    bbow[bbow!=0] = 1\n",
    "    print(\"Returning BBOW\")\n",
    "    return bbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frequency_bag_of_words(dataset, size, vocab_size):\n",
    "    fbow = np.zeros([size, vocab_size])\n",
    "    for index, review in enumerate(dataset):\n",
    "        word, frequency = np.unique(np.array(review[:-4].split(), dtype=int), return_counts=True)\n",
    "        for column, count in zip(word,frequency):\n",
    "            fbow[index][column-1] = count\n",
    "    return fbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Writing Dataset to csv_file\n",
    "def write_dataset_to_file(dataset, string):\n",
    "    file_name_str = \"./Created_Datasets/\" + str(string) + \".txt\"\n",
    "    np.savetxt(file_name_str, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating random assignment predictions for YELP\n",
    "def random_assignment_yelp(label_list):\n",
    "    labels = np.random.randint(5, size=len(label_list))\n",
    "    return np.array(labels, dtype= float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating random assignment predictions for IMDB\n",
    "def random_assignment_imdb(label_list):\n",
    "    labels = np.random.randint(2, size=len(label_list))\n",
    "    return np.array(labels, dtype= float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating majority class prediction for YELP\n",
    "def majority_assignment_yelp(label_list):\n",
    "    most_common = Counter(label_list).most_common(1)\n",
    "    most_common_class = most_common[0]\n",
    "    print(most_common_class[0])\n",
    "    return np.array(np.full(len(label_list), most_common_class[0]), dtype= float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train, raw_valid, raw_test, Y_train, Y_valid, Y_test = import_dataset('yelp')\n",
    "raw_train = np.array(raw_train)\n",
    "raw_valid = np.array(raw_valid)\n",
    "raw_test = np.array(raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### EXPORTING VOCABULARY FILE FOR YELP ###############\n",
    "export_vocab_file(np.append(raw_train, np.append(raw_valid , raw_test)), \"./Vocab/yelp-vocab.txt\")\n",
    "export_vocab_file(raw_train, \"./Vocab/yelp-train-vocab.txt\")\n",
    "export_vocab_file(raw_valid, \"./Vocab/yelp-valid-vocab.txt\")\n",
    "export_vocab_file(raw_test, \"./Vocab/yelp-test-vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### Exporting review as Indexes FOR YELP #####################\n",
    "generate_dataset_review_indexes(raw_train, \"./Generated_datasets/yelp-train.txt\", get_vocab(raw_train), Y_train)\n",
    "generate_dataset_review_indexes(raw_valid, \"./Generated_datasets/yelp-valid.txt\", get_vocab(raw_train), Y_valid)\n",
    "generate_dataset_review_indexes(raw_test, \"./Generated_datasets/yelp-test.txt\", get_vocab(raw_train), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### GENERATING BINARY BAG OF WORDS FOR YELP ##############################\n",
    "raw_train_file = open(\"./Generated_datasets/yelp-train.txt\",\"r\")\n",
    "raw_train_data = raw_train_file.readlines()\n",
    "X_train = frequency_bag_of_words(raw_train_data, len(raw_train), len(get_vocab(raw_train)))\n",
    "X_train = np.array(X_train, dtype = 'float')\n",
    "\n",
    "raw_valid_file = open(\"./Generated_datasets/yelp-valid.txt\",\"r\")\n",
    "raw_valid_data = raw_valid_file.readlines()\n",
    "X_valid = frequency_bag_of_words(raw_valid_data, len(raw_valid), len(get_vocab(raw_train)))\n",
    "X_valid = np.array(X_valid, dtype = 'float')\n",
    "\n",
    "raw_test_file = open(\"./Generated_datasets/yelp-test.txt\",\"r\")\n",
    "raw_test_data = raw_test_file.readlines()\n",
    "X_test = frequency_bag_of_words(raw_test_data, len(raw_test), len(get_vocab(raw_train)))\n",
    "X_test = np.array(X_test, dtype = 'float')\n",
    "\n",
    "Y_train = np.array(Y_train, dtype = float)\n",
    "Y_valid = np.array(Y_valid, dtype = float)\n",
    "Y_test = np.array(Y_test, dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_dataset_to_file(X_train, \"yelp_X_train\")\n",
    "write_dataset_to_file(Y_train, \"yelp_Y_train\")\n",
    "write_dataset_to_file(X_valid, \"yelp_X_valid\")\n",
    "write_dataset_to_file(Y_valid, \"yelp_Y_valid\")\n",
    "write_dataset_to_file(X_test, \"yelp_X_test\")\n",
    "write_dataset_to_file(Y_test, \"yelp_Y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test, Y_train, Y_valid, Y_test = import_created_dataset(\"yelp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train, raw_valid, raw_test, Y_train, Y_valid, Y_test = import_dataset('IMDB')\n",
    "raw_train = np.array(raw_train)\n",
    "raw_valid = np.array(raw_valid)\n",
    "raw_test = np.array(raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### EXPORTING VOCABULARY FILE FOR IMDB ###############\n",
    "export_vocab_file(np.append(raw_train, np.append(raw_valid , raw_test)), \"./Vocab/IMDB-vocab.txt\")\n",
    "export_vocab_file(raw_train, \"./Vocab/IMDB-train-vocab.txt\")\n",
    "export_vocab_file(raw_valid, \"./Vocab/IMDB-valid-vocab.txt\")\n",
    "export_vocab_file(raw_test, \"./Vocab/IMDB-test-vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### Exporting review as Indexes FOR IMDB #####################\n",
    "generate_dataset_review_indexes(raw_train, \"./Generated_datasets/IMDB-train.txt\", get_vocab(raw_train), Y_train)\n",
    "generate_dataset_review_indexes(raw_valid, \"./Generated_datasets/IMDB-valid.txt\", get_vocab(raw_train), Y_valid)\n",
    "generate_dataset_review_indexes(raw_test, \"./Generated_datasets/IMDB-test.txt\", get_vocab(raw_train), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning BBOW\n",
      "Returning BBOW\n",
      "Returning BBOW\n"
     ]
    }
   ],
   "source": [
    "##################### GENERATING FREQUENCY BAG OF WORDS FOR IMDB ##############################\n",
    "raw_train_file = open(\"./Generated_datasets/IMDB-train.txt\",\"r\")\n",
    "raw_train_data = raw_train_file.readlines()\n",
    "X_train = binary_bag_of_words(raw_train_data, len(raw_train), len(get_vocab(raw_train)))\n",
    "X_train = np.array(X_train, dtype = 'float')\n",
    "\n",
    "raw_valid_file = open(\"./Generated_datasets/IMDB-valid.txt\",\"r\")\n",
    "raw_valid_data = raw_valid_file.readlines()\n",
    "X_valid = binary_bag_of_words(raw_valid_data, len(raw_valid), len(get_vocab(raw_train)))\n",
    "X_valid = np.array(X_valid, dtype = 'float')\n",
    "\n",
    "raw_test_file = open(\"./Generated_datasets/IMDB-test.txt\",\"r\")\n",
    "raw_test_data = raw_test_file.readlines()\n",
    "X_test = binary_bag_of_words(raw_test_data, len(raw_test), len(get_vocab(raw_train)))\n",
    "X_test = np.array(X_test, dtype = 'float')\n",
    "\n",
    "Y_train = np.array(Y_train, dtype = float)\n",
    "Y_valid = np.array(Y_valid, dtype = float)\n",
    "Y_test = np.array(Y_test, dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_dataset_to_file(X_train, \"IMDB_X_train\")\n",
    "write_dataset_to_file(Y_train, \"IMDB_Y_train\")\n",
    "write_dataset_to_file(X_valid, \"IMDB_X_valid\")\n",
    "write_dataset_to_file(Y_valid, \"IMDB_Y_valid\")\n",
    "write_dataset_to_file(X_test, \"IMDB_X_test\")\n",
    "write_dataset_to_file(Y_test, \"IMDB_Y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test, Y_train, Y_valid, Y_test = import_created_dataset(\"IMDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "for i in range(1,10):\n",
    "    tree_classifier = tree.DecisionTreeClassifier(max_depth= 100+ i*10, min_samples_leaf= 8)\n",
    "    tree_classifier = tree_classifier.fit(X_train, Y_train)\n",
    "    tree_pred = tree_classifier.predict(X_valid)\n",
    "    #tree_score = f1_score(Y_valid, tree_pred, average= 'micro', labels= [1,2,3,4,5])\n",
    "    tree_score = f1_score(Y_valid, tree_pred)\n",
    "    score_str = \"Min Samples Leaf = \" + str(100+ i*10) + \" F-Score: \" + str(tree_score)\n",
    "    print(score_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Classifier\n",
    "for i in range(1,10):\n",
    "    svm_classifier = svm.SVC(C = 1 * i)\n",
    "    svm_classifier = svm_classifier.fit(X_train, Y_train)\n",
    "    svm_pred = svm_classifier.predict(X_valid)\n",
    "    #svm_score = f1_score(Y_valid, svm_pred, average= 'micro', labels= [1,2,3,4,5])\n",
    "    svm_score = f1_score(Y_valid, svm_pred)\n",
    "    score_str = \"C = \" + str(i) + \" F-Score: \" + str(svm_score)\n",
    "    print(score_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classifier (Bernoulli)\n",
    "for i in range(0,10):\n",
    "    nb_classifier = BernoulliNB(alpha= i)\n",
    "    nb_classifier = nb_classifier.fit(X_train, Y_train)\n",
    "    nb_pred = nb_classifier.predict(X_valid)\n",
    "    #nb_score = f1_score(Y_valid, nb_pred, average= 'micro', labels= [1,2,3,4,5])\n",
    "    nb_score = f1_score(Y_valid, nb_pred)\n",
    "    score_str = \"Alpha: \" + str(i*10) + \" F-Score: \" + str(nb_score)\n",
    "    print(score_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classifier (Gaussian)\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier = nb_classifier.fit(X_train, Y_train)\n",
    "nb_pred = nb_classifier.predict(X_valid)\n",
    "nb_score = f1_score(Y_valid, nb_pred)#, average= 'micro', labels= [1,2,3,4,5])\n",
    "print(nb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50260173449\n",
      "0.500747831289\n",
      "0.501238118061\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(Y_train, random_assignment_imdb(Y_train)))\n",
    "print(f1_score(Y_valid, random_assignment_imdb(Y_valid)))\n",
    "print(f1_score(Y_test, random_assignment_imdb(Y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
